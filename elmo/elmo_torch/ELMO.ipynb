{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial of implementing Elmo with PyTorch\n",
    "\n",
    "- Author:  _YuriAntonovsky@QuantumAgent_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook,tqdm_pandas\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import os\n",
    "lmap=lambda func,it: list(map(lambda x:func(x),it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "n = lambda x: nlp(x, disable=['tagger', 'ner', 'textcat', 'parser'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Preprocessing\n",
    "\n",
    "- (one-hot)tize characters\n",
    "- build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles=pd.read_csv('data/title_text.csv').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "characters=defaultdict()\n",
    "characters.setdefault('',len(characters))\n",
    "for c in \" abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\":\n",
    "    characters.setdefault(c,len(characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/pandas/core/internals.py:3462: FutureWarning: Passing in 'datetime64' dtype with no frequency is deprecated and will raise in a future version. Please pass in 'datetime64[ns]' instead.\n",
      "  return self.apply('astype', dtype=dtype, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "titles['date']=titles['date'].astype(np.datetime64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles.drop('Unnamed: 0',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=list(titles['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=lmap(lambda x:x.strip().lower(),data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocabulary = defaultdict()\n",
    "vocabulary.setdefault('',len(vocabulary))\n",
    "vocabulary.setdefault('<SOS>',len(vocabulary))\n",
    "vocabulary.setdefault('<EOS>',len(vocabulary))\n",
    "preprocessed_data=[]\n",
    "title_date=[]\n",
    "for i,d in tqdm_notebook(enumerate(data)):\n",
    "    if np.array(lmap(lambda x:x in characters ,list(d))).prod() == 0: continue\n",
    "    dn=n(d)\n",
    "    tokens=lmap(lambda x:x.text,dn)\n",
    "    word_index=[]\n",
    "    chars=[]\n",
    "    word_index.append(vocabulary['<SOS>'])\n",
    "    for t in tokens:\n",
    "        t_text=t.strip()\n",
    "        char_index=[]\n",
    "        vocabulary.setdefault(t_text, len(vocabulary))\n",
    "        word_index.append(vocabulary[t_text])\n",
    "        for c in list(t_text):\n",
    "            char_index.append(characters[c])\n",
    "        chars.append(char_index)\n",
    "    word_index.append(vocabulary['<EOS>'])\n",
    "    preprocessed_data.append((word_index,chars))\n",
    "    title_date.append(titles['date'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_date=np.array(title_date)\n",
    "\n",
    "np.save('./data/title_date',title_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94297"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/vocabulary.pkl','wb+') as f:\n",
    "    pickle.dump(vocabulary,f)\n",
    "\n",
    "with open('./data/characters.pkl','wb+') as f:\n",
    "    pickle.dump(characters,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokens=lmap(lambda x:x[0],preprocessed_data)\n",
    "\n",
    "corpus_chars=lmap(lambda x:x[1],preprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (lmap(lambda x:len(x),corpus_tokens) ==lmap(lambda x:len(x)+2,corpus_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_char_length=max(lmap(lambda y:max(lmap(lambda x:len(x),y)),corpus_chars))\n",
    "\n",
    "max_token_length=max(lmap(lambda x:len(x),corpus_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct one-hot embedding\n",
    "onehot=np.eye(len(characters))\n",
    "onehot[0,0]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_pad(max_leng, word_index):\n",
    "    if len(word_index) > max_leng:\n",
    "        return word_index[:max_leng]\n",
    "    pad_leng = max_leng - len(word_index)\n",
    "    word_index = word_index + [0] * pad_leng\n",
    "    assert len(word_index) == max_leng\n",
    "    return word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=432006), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pad_tokens=[]\n",
    "for t in tqdm_notebook(corpus_tokens):\n",
    "    pad_tokens.append(crop_pad(max_leng=max_token_length,word_index=t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_tokens=np.array(pad_tokens)\n",
    "\n",
    "corpus_tokens=pad_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(432006, 40)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=432006), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pad_chars=[]\n",
    "for s in tqdm_notebook(corpus_chars):\n",
    "    pad_sent=[]\n",
    "    for w in s:\n",
    "        pad_sent.append(crop_pad(max_leng=max_char_length,word_index=w))\n",
    "    if len(s)<max_token_length:\n",
    "        pad_leng=max_token_length-len(s)\n",
    "        pad_sent=pad_sent+([[0] * max_char_length] * pad_leng)\n",
    "    pad_chars.append(pad_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_chars=np.array(pad_chars)\n",
    "\n",
    "corpus_chars=pad_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/corpus_tokens',corpus_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/corpus_chars',corpus_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Embedding\n",
    "- kernel_size: 2,3,4,5\n",
    "- filters: 32,32,32,32\n",
    "- word_vector_length: $\\sum{filters}=128$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterEmbedding(nn.Module):\n",
    "    def __init__(self, weight_matrix, dropout, filters=[32, 32, 32, 32], kernel_sizes=[2, 3, 4, 5]):\n",
    "        super(CharacterEmbedding, self).__init__()\n",
    "        input_size = weight_matrix.shape[1]\n",
    "        self.output_size = sum(filters)\n",
    "        self.embedding = nn.Embedding(input_size, input_size, _weight=weight_matrix)\n",
    "        self.convs = nn.ModuleList([nn.Conv1d(in_channels=input_size, out_channels=filters[i], kernel_size=(1, kernel_sizes[i])) for i in range(len(filters))])\n",
    "        self.highway_h = nn.Linear(sum(filters), sum(filters))\n",
    "        self.highway_t = nn.Linear(sum(filters), sum(filters))\n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def conv_and_pool(self, x, conv):\n",
    "        b, w, c, e = tuple(x.shape)\n",
    "        x_out = conv(x.view(b, e, w, c)).max(dim=-1)[0]\n",
    "        return x_out.view(b, w, -1)\n",
    "    \n",
    "    def highway(self, x):\n",
    "        x_h = F.relu(self.highway_h(x))\n",
    "        x_t = F.sigmoid(self.highway_t(x))\n",
    "        x_out = x_h * x_t + x * (1 - x_t)\n",
    "        return x_out\n",
    "    \n",
    "    def forward(self, x, train=False):\n",
    "        x = self.embedding(x)\n",
    "        results = list(map(lambda conv: self.conv_and_pool(x, conv), self.convs))\n",
    "        results = torch.cat(results, dim=-1)\n",
    "        results = self.highway(results)\n",
    "        if train:\n",
    "            results = self.dropout(results)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-Language-Model\n",
    "- #layer=2\n",
    "- bidirectional LSTM\n",
    "- hidden_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, word_number, dropout):\n",
    "        super(BiLM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, bidirectional=True)\n",
    "        self.out_fc = nn.Linear(2 * hidden_size, word_number)\n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def forward(self, x, hidden=None, train=False):\n",
    "        out = x\n",
    "        if train:\n",
    "            out = self.dropout(out)\n",
    "        out, (h_n, h_c) = self.lstm(out, hidden)\n",
    "        out = F.log_softmax(F.relu(self.out_fc(out)), dim=-1)\n",
    "        return out, (h_n, h_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Elmo\n",
    "- use word vector $w_t$ as input where $w_t \\in R^{128} $\n",
    "- ${h_c}^{l}=[{h_{cf}}^{l};{h_{cb}}^{l}]$\n",
    "- ${h_n}^{l}=[{h_{cf}}^{l};{h_{cb}}^{l}]$\n",
    "- ${h}^{l}=[{h_n}^{l};{h_c}^l]$\n",
    "- $h=\\sum_l{s_l}{h^l}$\n",
    "- $\\sum_l{s_l}=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Elmo(object):\n",
    "    def __init__(self, weight_matrix, word_number, hidden_size=64, num_layers=2, learning_rate=1e-3, dp=0.2):\n",
    "        super(Elmo, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dp)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.char_emb = CharacterEmbedding(weight_matrix=torch.tensor(weight_matrix, dtype=torch.float32), dropout=self.dropout)\n",
    "        self.bilm = BiLM(input_size=self.char_emb.output_size, hidden_size=self.hidden_size, num_layers=self.num_layers, word_number=word_number, dropout=self.dropout)\n",
    "        self.optimizer = optim.Adam(list(self.char_emb.parameters()) + list(self.bilm.parameters()), lr=learning_rate)\n",
    "        self.criterion = nn.NLLLoss()\n",
    "    \n",
    "    def _encode(self, x, gamma=1):\n",
    "        # weight vector of every layer not implemented!\n",
    "        max_word_length = (((x > 0).sum(axis=-1)) > 0).sum(axis=1).max()\n",
    "        x_w = x[:, :max_word_length, :]\n",
    "        x_w = self.char_emb(torch.tensor(x_w), train=False)\n",
    "        _, (hn, hc) = self.bilm(torch.tensor(x_w), train=False)\n",
    "        hns = []\n",
    "        hcs = []\n",
    "        for i in range(0, self.num_layers * 2, 2):\n",
    "            hns.append(torch.cat([hn[i, :, :], hn[i + 1, :, :]], dim=-1))\n",
    "            hcs.append(torch.cat([hc[i, :, :], hc[i + 1, :, :]], dim=-1))\n",
    "        # same weight for each layer\n",
    "        hns = torch.stack(hns).mean(0)\n",
    "        hcs = torch.stack(hcs).mean(0)\n",
    "        \n",
    "        encode_result = torch.cat([hns, hcs], dim=-1)\n",
    "        return encode_result.detach().numpy()\n",
    "    \n",
    "    def encode(self, X, batch_size=64, gamma=1):\n",
    "        pointer = 0\n",
    "        results_ = np.zeros((1, self.hidden_size*4))\n",
    "        while pointer < X.shape[0]:\n",
    "            batch_x = X[pointer:(pointer + batch_size)]\n",
    "            result_batch = self._encode(batch_x, gamma=gamma)\n",
    "            results_ = np.concatenate((results_, result_batch))\n",
    "            pointer += batch_size\n",
    "        return results_[1:]\n",
    "    \n",
    "    def _train(self, x, y):\n",
    "        max_word_length = (y > 0).sum(axis=1).max()\n",
    "        batch_x = x[:, :max_word_length, :]\n",
    "        batch_y = y[:, :max_word_length]\n",
    "        self.optimizer.zero_grad()\n",
    "        y_f = batch_y[:, 1:]\n",
    "        y_b = batch_y[:, :-1]\n",
    "        x_w = self.char_emb(torch.tensor(batch_x), train=True)\n",
    "        y_hat, (_, _) = self.bilm(x_w, train=True)\n",
    "        y_hat_f = y_hat[:, :-1, :]\n",
    "        y_hat_b = y_hat[:, 1:, :]\n",
    "        loss_f = self.criterion(y_hat_f.transpose(1, -1), torch.tensor(y_f))\n",
    "        loss_b = self.criterion(y_hat_b.transpose(1, -1), torch.tensor(y_b))\n",
    "        loss = (loss_f + loss_b) / 2\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "    \n",
    "    def train(self, X, y, batch_size=64, epoch=2):\n",
    "        global_step = 0\n",
    "        for e in range(epoch):\n",
    "            pointer = 0\n",
    "            indices = np.arange(X.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "            while pointer < X.shape[0]:\n",
    "                batch_x = X_shuffled[pointer:(pointer + batch_size)]\n",
    "                batch_y = y_shuffled[pointer:(pointer + batch_size)]\n",
    "                mean_loss = self._train(batch_x, batch_y)\n",
    "                print(mean_loss, 'batch%:', round((pointer / X.shape[0]) * 100, 4), 'epoch:', e)\n",
    "                pointer += batch_size\n",
    "#                 writer.add_scalar(tag='loss', scalar_value=mean_loss, global_step=global_step)\n",
    "                global_step += 1\n",
    "            self.save_model()\n",
    "    \n",
    "    def save_model(self, model_path='./ELMO'):\n",
    "        if not os.path.exists(model_path):\n",
    "            os.mkdir(model_path)\n",
    "        print(\"saving models\")\n",
    "        torch.save(self.char_emb, model_path + '/char_emb.pkl')\n",
    "        torch.save(self.bilm, model_path + '/bilm.pkl')\n",
    "    \n",
    "    def load_model(self, model_path='./ELMO'):\n",
    "        print(\"loading models\")\n",
    "        self.char_emb = torch.load(model_path + '/char_emb.pkl')\n",
    "        self.bilm = torch.load(model_path + '/bilm.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment\n",
    "- sample size: 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x=corpus_chars[:128,:,:]\n",
    "\n",
    "batch_y=corpus_tokens[:128,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 40, 30)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 40)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo=Elmo(weight_matrix=onehot,word_number=len(vocabulary),learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.464266777038574 batch%: 0.0 epoch: 0\n",
      "11.46330738067627 batch%: 50.0 epoch: 0\n",
      "saving models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type CharacterEmbedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.5/dist-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type BiLM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.452033996582031 batch%: 0.0 epoch: 1\n",
      "11.451211929321289 batch%: 50.0 epoch: 1\n",
      "saving models\n",
      "CPU times: user 1min, sys: 7.91 s, total: 1min 8s\n",
      "Wall time: 20.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "elmo.train(X=batch_x,y=batch_y,batch_size=64,epoch=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading models\n"
     ]
    }
   ],
   "source": [
    "elmo.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results=elmo.encode(X=batch_x,batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seems OK, Thank you"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}