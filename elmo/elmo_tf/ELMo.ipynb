{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ELMo.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "0EK5W_jrfW_B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preprocess"
      ]
    },
    {
      "metadata": {
        "id": "dp26aJzNJ0Xt",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J70q1PwwQdnI",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FITGhgAih0AU",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RCUbupFg7N_X",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "imdb = keras.datasets.imdb\n",
        "(train_text, _), (test_text, _) = imdb.load_data(num_words=10000)\n",
        "\n",
        "# A dictionary mapping words to an integer index\n",
        "word_index = imdb.get_word_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NcndpTps-pB7",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# The first indices are reserved\n",
        "vocab = {k: (v + 3) for k, v in word_index.items()} \n",
        "vocab[\"<PAD>\"] = 0\n",
        "vocab[\"<START>\"] = 1\n",
        "vocab[\"<UNK>\"] = 2  # unknown\n",
        "vocab[\"<UNUSED>\"] = 3\n",
        "vocab[\"<END>\"] = len(vocab)\n",
        "\n",
        "reverse_word_index = dict([(value, key) for (key, value) in vocab.items()])\n",
        "\n",
        "train_X, train_y = [], []\n",
        "for t in train_text:\n",
        "  train_X.append(t[1:])\n",
        "  train_y.append(t + [vocab[\"<END>\"]])\n",
        "  \n",
        "test_X, test_y = [], []\n",
        "for t in test_text:\n",
        "  test_X.append(t[1:])\n",
        "  test_y.append(t + [vocab[\"<END>\"]])\n",
        "\n",
        "print(\"Training entries: {}\".format(len(train_X)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nQX-qQ2GiQGo",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "train_X = keras.preprocessing.sequence.pad_sequences(train_X,\n",
        "                                                    value=vocab[\"<PAD>\"],\n",
        "                                                    padding='post',\n",
        "                                                    maxlen=256)\n",
        "\n",
        "train_y = keras.preprocessing.sequence.pad_sequences(train_y,\n",
        "                                                    value=vocab[\"<PAD>\"],\n",
        "                                                    padding='post',\n",
        "                                                    maxlen=256)\n",
        "\n",
        "test_X = keras.preprocessing.sequence.pad_sequences(test_X,\n",
        "                                                   value=vocab[\"<PAD>\"],\n",
        "                                                   padding='post',\n",
        "                                                   maxlen=256)\n",
        "\n",
        "test_y = keras.preprocessing.sequence.pad_sequences(test_y,\n",
        "                                                   value=vocab[\"<PAD>\"],\n",
        "                                                   padding='post',\n",
        "                                                   maxlen=256)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c7mgG3zmCdXF",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "  \n",
        "print(decode_review(train_X[0]))\n",
        "print(decode_review(train_y[0]))\n",
        "print(decode_review(train_text[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JgaMjR2IT28F",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(len(train_X[0]), len(train_y[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pXz7qTzB9pk7",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "chars = \"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
        "alphabet = {c: i for i, c in enumerate(chars)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7zZtXmlFjQHm",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "temp_data = []\n",
        "\n",
        "for text in train_X:\n",
        "  sent = []\n",
        "  \n",
        "  for word_index in text:\n",
        "    word = []\n",
        "    token = reverse_word_index[word_index]\n",
        "    \n",
        "    if token in {\"<PAD>\", \"<UNK>\", \"<UNUSED>\"}:\n",
        "      word.append(0)\n",
        "    else:\n",
        "      for char in token:\n",
        "        if not char in alphabet:\n",
        "          continue\n",
        "        word.append(alphabet[char])\n",
        "      \n",
        "    sent.append(word)\n",
        "    \n",
        "  temp_data.append(sent)\n",
        "  \n",
        "train_X = temp_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Me7ECT2cMoGL",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "temp_data = []\n",
        "\n",
        "for sent in train_X:\n",
        "  temp_data.append(keras.preprocessing.sequence.pad_sequences(sent,\n",
        "                                                              value=0,\n",
        "                                                              padding='post',\n",
        "                                                              maxlen=16))\n",
        "\n",
        "train_X = np.asarray(temp_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WMEJ3WERJEmy",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(train_X.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JkE_lxF3cPNL",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "config = {\"SENT_LENGTH\": 256,\n",
        "          \"CHAR_LENGTH\": 16,\n",
        "          \"NUM_RNN_LAYER\": 2,\n",
        "          \"RNN_LAYER_NODES\": [\n",
        "              64,\n",
        "              64\n",
        "          ]\n",
        "         }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nUGJh9unaEN0",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class ELMoModel:\n",
        "  def __init__(self, params):\n",
        "    self.params = params\n",
        "    tf.reset_default_graph()\n",
        "    self._create_placeholder()\n",
        "    self._create_model()\n",
        "    self._create_loss()\n",
        "    self._create_optimizer()\n",
        "    self.sess = tf.Session()\n",
        "  \n",
        "  \n",
        "  def _create_placeholder(self):\n",
        "    with tf.name_scope(\"input\"):\n",
        "      self.input_text = tf.placeholder(tf.int32, \n",
        "                                       shape=[None, self.params[\"SENT_LENGTH\"], self.params[\"CHAR_LENGTH\"]], \n",
        "                                       name=\"input_text\")\n",
        "      self.dropout_keep_prob = tf.placeholder(tf.float64, name=\"dropout_keep_prob\")\n",
        "      self.y = tf.placeholder(tf.int32, \n",
        "                             shape=[None, self.params[\"SENT_LENGTH\"]], \n",
        "                             name=\"input_label\")\n",
        "  \n",
        "  \n",
        "  def _create_model(self):\n",
        "    char_emb = np.eye((len(alphabet)))\n",
        "    char_emb[0, 0] = 0\n",
        "\n",
        "    with tf.name_scope(\"char_embedding\"):\n",
        "      \n",
        "      char_emb_tensor = tf.Variable(char_emb)\n",
        "      emb_text = tf.nn.embedding_lookup(char_emb_tensor, self.input_text[:, :-2, :])\n",
        "      emb_text = tf.expand_dims(emb_text, -1)\n",
        "      emb_text_word = tf.unstack(emb_text, axis=1)\n",
        "      print(emb_text.shape)\n",
        "    \n",
        "    with tf.name_scope(\"conv_layer\") as scope:\n",
        "\n",
        "      print(\"------- build conv layer --------\")\n",
        "\n",
        "      two_gram = [tf.squeeze(self.conv_2d(x, 2, len(alphabet), 3, scope + 'two_gram'), axis=2) for x in emb_text_word]\n",
        "      two_gram = [tf.reduce_max(x, axis=1) for x in two_gram]\n",
        "\n",
        "      three_gram = [tf.squeeze(self.conv_2d(x, 3, len(alphabet), 4, scope + 'three_gram'), axis=2) for x in emb_text_word]\n",
        "      three_gram = [tf.reduce_max(x, axis=1) for x in three_gram]\n",
        "\n",
        "      four_gram = [tf.squeeze(self.conv_2d(x, 4, len(alphabet), 5, scope + 'four_gram'), axis=2) for x in emb_text_word]\n",
        "      four_gram = [tf.reduce_max(x, axis=1) for x in four_gram]\n",
        "\n",
        "      five_gram = [tf.squeeze(self.conv_2d(x, 5, len(alphabet), 6, scope + 'five_gram'), axis=2) for x in emb_text_word]\n",
        "      five_gram = [tf.reduce_max(x, axis=1) for x in five_gram]\n",
        "\n",
        "      emb_text_word = [tf.concat(out, 1) for out in zip(two_gram, three_gram, four_gram, five_gram)]\n",
        "      print(emb_text_word[0].shape)\n",
        "      emb_text = tf.stack(emb_text_word, axis=1)\n",
        "        \n",
        "    with tf.name_scope(\"rnn_layer\") as scope:\n",
        "      \n",
        "      print(\"------- build RNN layer --------\")\n",
        "      \n",
        "      lstm_cell_fw = tf.nn.rnn_cell.MultiRNNCell(\n",
        "          [tf.contrib.rnn.LSTMCell(self.params[\"RNN_LAYER_NODES\"][i], name=\"rnn_cell_fw_\" + str(i)) for i in range(self.params[\"NUM_RNN_LAYER\"])])\n",
        "      lstm_cell_bw = tf.nn.rnn_cell.MultiRNNCell(\n",
        "          [tf.contrib.rnn.LSTMCell(self.params[\"RNN_LAYER_NODES\"][i], name=\"rnn_cell_bw_\" + str(i)) for i in range(self.params[\"NUM_RNN_LAYER\"])])\n",
        "      \n",
        "      lstm_out, self.hidden_state = tf.nn.bidirectional_dynamic_rnn(cell_fw=lstm_cell_fw,\n",
        "                                                                    cell_bw=lstm_cell_bw,\n",
        "                                                                    inputs=emb_text,\n",
        "                                                                    dtype=tf.float64)\n",
        "      \n",
        "      fw_out = tf.layers.dense(inputs=lstm_out[0], units=len(vocab), activation=tf.nn.relu)\n",
        "      bw_out = tf.layers.dense(inputs=lstm_out[1], units=len(vocab), activation=tf.nn.relu)\n",
        "      \n",
        "      self.out_1 = tf.nn.softmax(fw_out, axis=-1)\n",
        "      self.out_2 = tf.nn.softmax(bw_out, axis=-1)\n",
        "\n",
        "  \n",
        "  def _create_loss(self):\n",
        "    \n",
        "    with tf.name_scope(\"loss\") as scope:\n",
        "      loss_fw = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.out_1, labels=self.y[:, 2:])\n",
        "      loss_bw = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.out_2, labels=self.y[:, :-2])\n",
        "\n",
        "      self.loss = tf.reduce_mean(loss_fw + loss_bw)\n",
        "  \n",
        "  \n",
        "  def _create_optimizer(self):\n",
        "    self.optimizer = tf.train.AdamOptimizer(learning_rate=1e-2).minimize(self.loss)\n",
        "    \n",
        "    \n",
        "  def save_model(self):\n",
        "    pass\n",
        "  \n",
        "  def conv_2d(self, input_, height, width, out, scope):\n",
        "    return tf.contrib.layers.conv2d(input_, out, [height, width], padding=\"VALID\", reuse=tf.AUTO_REUSE, scope=scope)\n",
        "  \n",
        "  def train(self, train_X, train_y, batch_size):\n",
        "    \n",
        "      self.sess.run(tf.global_variables_initializer())\n",
        "  \n",
        "      global_step = 0\n",
        "      for epoch in range(5):\n",
        "        batch = 0\n",
        "        while batch < len(train_X):\n",
        "          _, loss = self.sess.run([self.optimizer, self.loss], \n",
        "                             feed_dict={self.input_text: train_X[batch: batch + batch_size],\n",
        "                                        self.y: train_y[batch: batch + batch_size]\n",
        "                                       }\n",
        "                            )\n",
        "          batch += batch_size\n",
        "          global_step += 1\n",
        "          print(\"step {}, loss {}\".format(global_step, loss))\n",
        "    \n",
        "    \n",
        "model = ELMoModel(config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WBfIA8SGNJvM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model.train(train_X, train_y, 4)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}